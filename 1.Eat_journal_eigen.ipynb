{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import xlrd\n",
    "from sklearn.preprocessing import normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from pandas import DataFrame\n",
    "import scipy as sc\n",
    "from scipy import io\n",
    "from scipy.stats import pearsonr\n",
    "from os.path import join, exists, dirname, basename\n",
    "from glob import glob\n",
    "from brainspace import gradient\n",
    "from random import randint\n",
    "import nibabel as nib\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-drunk",
   "metadata": {},
   "source": [
    "# Load HCP file, BNA atlas mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nibabel as nib\n",
    "\n",
    "\n",
    "Atlas = nib.load(join(atlas_path,'BNA_2mm.nii')).get_fdata()\n",
    "n_roi = int(np.max(Atlas))\n",
    "print(n_roi)\n",
    "print(Atlas.shape)\n",
    "\n",
    "save_ts = False\n",
    "\n",
    "for p in path_subj:\n",
    "    print(f'{path_subj.index(p)} / {len(path_subj)}')\n",
    "\n",
    "    subj_id = basename(p)\n",
    "    path_REST = glob(join(p,'MNINonLinear/Results/rfMRI_REST*'))\n",
    "    \n",
    "    for f in path_REST:\n",
    "        start_time = time.time()\n",
    "        save_path = f\n",
    "        \n",
    "        print(f' {subj_id} {basename(f)}')\n",
    "        \n",
    "        nii_file = glob(join(f,'*hp2000_clean.nii.gz'))\n",
    "        fmri = nib.load(nii_file[0]).get_fdata()\n",
    "\n",
    "        ts = np.array([np.mean(fmri[Atlas==roi], axis=0) for roi in range(1, n_roi+1)]) \n",
    "        if save_ts:\n",
    "            print(ts.shape)\n",
    "            np.save(join(save_path, f'ts_BNA.npy'), ts)\n",
    "            print(' save ts')\n",
    "        \n",
    "        print(f' {time.time() - start_time} seconds!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-progress",
   "metadata": {},
   "source": [
    "# Load HCP BNA time series, demean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-drink",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import nibabel as nib\n",
    "\n",
    "for p in path_subj:\n",
    "    print(f'{path_subj.index(p)} / {len(path_subj)}')\n",
    "\n",
    "    subj_id = basename(p)\n",
    "    path_REST = glob(join(p,'MNINonLinear/Results/rfMRI_REST*'))\n",
    "    \n",
    "    for f in path_REST:\n",
    "        start_time = time.time()\n",
    "        save_path = f\n",
    "        \n",
    "        print(f' {subj_id} {basename(f)}')\n",
    "        \n",
    "        ts_file = glob(join(f, 'ts_BNA.npy'))\n",
    "        ts = np.load(ts_file[0])\n",
    "        ts_demean = np.subtract(ts, ts.mean(axis=1).reshape(-1,1))\n",
    "        np.save(join(save_path,'ts_demean_BNA.npy'),ts_demean)\n",
    "        print(' save ts demean')\n",
    "        print(f' {time.time() - start_time} seconds!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-recycling",
   "metadata": {},
   "source": [
    "# Concat HCP not twin times series, make FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_demo = pd.read_excel(join(main_path, 'S1200_total.xlsx'), sheet_name='NotTwin_QC', skiprows=0)\n",
    "\n",
    "sub_list_hcp = template_demo['Subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list_hcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-sampling",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check REST quality\n",
    "\n",
    "for i in sub_list_hcp:\n",
    "    REST_list = glob(join(path_subj, f'{i}/MNINonLinear/Results/*'))\n",
    "    \n",
    "    if len(REST_list) !=4:\n",
    "        print(i, len(REST_list))\n",
    "print('QC finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-windows",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for i in sub_list_hcp:\n",
    "    print(f'{np.where(sub_list == i)[0][0]} / {len(sub_list_hcp)}, {i}')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    folder_name = str(i)\n",
    "    if not(os.path.isdir(join(save_path,f'{folder_name}'))):\n",
    "        os.makedirs(os.path.join(save_path,f'{folder_name}'))\n",
    "    \n",
    "    REST_list = sorted(glob(join(path_subj, f'{i}/MNINonLinear/Results/*')))\n",
    "    LR1 = np.load(f'{REST_list[0]}/ts_demean_BNA.npy')\n",
    "    RL1 = np.load(f'{REST_list[1]}/ts_demean_BNA.npy')\n",
    "    LR2 = np.load(f'{REST_list[2]}/ts_demean_BNA.npy')\n",
    "    RL2 = np.load(f'{REST_list[3]}/ts_demean_BNA.npy')\n",
    "    \n",
    "    ts_demean_stacked = np.concatenate((LR1,RL1,LR2,RL2), axis=1)\n",
    "    conn_mat =np.nan_to_num(np.where(np.eye(n_roi) ==1, 0, np.corrcoef(ts_demean_stacked)))\n",
    "    conn_mat_rtoz =  np.nan_to_num(np.arctanh(conn_mat),nan=0.0)\n",
    "    \n",
    "    np.save(join(save_path,folder_name,'ts_demean_stacked_BNA.npy'), ts_demean_stacked)\n",
    "    np.save(join(save_path,folder_name,'conn_mat_pear_BNA.npy'), conn_mat)\n",
    "    np.save(join(save_path,folder_name,'conn_mat_pear_rtoz_BNA.npy'), conn_mat_rtoz)\n",
    "    \n",
    "    print(' ts stacked')\n",
    "    print(' conn mat')\n",
    "    print(' conn mat r to z')\n",
    "    \n",
    "    print(f'{time.time() - start_time} seconds!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-miniature",
   "metadata": {},
   "source": [
    "# Make gradient (eigenvector) template (using HCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only surface cortex\n",
    "file_list_hcp = [np.load(join(save_path, str(i), 'conn_mat_pear_BNA.npy'))[:210,:210] for i in sub_list_hcp]\n",
    "    \n",
    "grp_conn_mat = np.array(file_list_hcp).mean(axis=0)\n",
    "grp_conn_mat_rtoz = np.nan_to_num(np.arctanh(grp_conn_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC Figure\n",
    "plt.figure(figsize = (5,5)) # (20,20)\n",
    "plt.matshow(grp_conn_mat,  fignum=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-bronze",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sparsity in [0.5, 0.8, 0.75, 0.7, 0.6]:\n",
    "\n",
    "#     sparsity = 0.9\n",
    "\n",
    "    k = str(int(100-sparsity*100))\n",
    "    k = k.zfill(2)\n",
    "\n",
    "    print(f'Top {k}')\n",
    "\n",
    "    noaff_grp_conn_mat = gradient.compute_affinity(grp_conn_mat_rtoz, sparsity = sparsity)\n",
    "    aff_grp_conn_mat = gradient.compute_affinity(grp_conn_mat_rtoz, kernel = 'cosine', sparsity = sparsity)\n",
    "\n",
    "    print(f'noaff isnan : {np.isnan(noaff_grp_conn_mat).sum()}, symmetric : {gradient.is_symmetric(noaff_grp_conn_mat)}')\n",
    "    print(f'aff isnan : {np.isnan(aff_grp_conn_mat).sum()}, symmetric : {gradient.is_symmetric(aff_grp_conn_mat)}')\n",
    "\n",
    "    # rmsubcor = remove subcortical cortex\n",
    "\n",
    "    np.save(join(save_path, f'noaff_grp_conn_mat_pear_top{k}_BNA_rmsubcor.npy'), noaff_grp_conn_mat)\n",
    "    np.save(join(save_path, f'aff_grp_conn_mat_pear_top{k}_BNA_rmsubcor.npy'), aff_grp_conn_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_num = 10\n",
    "\n",
    "# grp_aff = np.load(join(save_path, f'noaff_grp_conn_mat_pear_top10_BNA_rmsubcor.npy'))\n",
    "grp_aff = np.load(join(save_path, f'aff_grp_conn_mat_pear_top10_BNA_rmsubcor.npy'))\n",
    "\n",
    "# emb = gradient.embedding.PCAMaps(n_components = comp_num)\n",
    "emb = gradient.embedding.DiffusionMaps(n_components = comp_num)\n",
    "\n",
    "emb.fit(grp_aff)\n",
    "ref_lam = emb.lambdas_ \n",
    "ref_PC = emb.maps_\n",
    "\n",
    "grad1 = ref_PC[:,0]\n",
    "grad2 = ref_PC[:,1]\n",
    "grad3 = ref_PC[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-greenhouse",
   "metadata": {},
   "source": [
    "# Load Validation datset (Lemon) time series, demean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-brave",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "Atlas = nib.load(join(atlas_path,'BNA_2mm.nii')).get_fdata()\n",
    "n_roi = int(np.max(Atlas))\n",
    "print(n_roi)\n",
    "print(Atlas.shape)\n",
    "\n",
    "save_ts = True\n",
    "\n",
    "for p in path_subj[156:]:\n",
    "    print(f'{path_subj.index(p)} / {len(path_subj)}')\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    subj_id = basename(p)\n",
    "    folder_name = subj_id\n",
    "    print(subj_id)\n",
    "    \n",
    "    nii_file = glob(join(p, 'func','*MNI2mm*'))\n",
    "    \n",
    "    if not nii_file:\n",
    "        print('Pass')\n",
    "        continue\n",
    "    \n",
    "    if not (os.path.isdir(os.path.join(save_path_lemon,f'{folder_name}'))):\n",
    "            os.makedirs(os.path.join(save_path_lemon,f'{folder_name}'))\n",
    "    \n",
    "    \n",
    "    fmri = nib.load(nii_file[0]).get_fdata()\n",
    "    \n",
    "    ts = np.array([np.mean(fmri[Atlas==roi], axis=0) for roi in range(1, n_roi+1)]) \n",
    "    ts_demean = np.subtract(ts, ts.mean(axis=1).reshape(-1,1))\n",
    "    \n",
    "    conn_mat =np.nan_to_num(np.where(np.eye(n_roi) ==1, 0, np.corrcoef(ts_demean)))\n",
    "    conn_mat_rtoz =  np.nan_to_num(np.arctanh(conn_mat),nan=0.0)\n",
    "        \n",
    "    if save_ts:\n",
    "        print(ts.shape)\n",
    "        np.save(join(save_path_lemon,folder_name, f'ts_BNA.npy'), ts)\n",
    "        print(' save ts')\n",
    "        np.save(join(save_path_lemon,folder_name,'ts_demean_BNA.npy'),ts_demean)\n",
    "        print(' save ts demean')\n",
    "        np.save(join(save_path_lemon,folder_name,'conn_mat_pear_BNA.npy'), conn_mat)\n",
    "        print(' conn mat')\n",
    "        np.save(join(save_path_lemon,folder_name,'conn_mat_pear_rtoz_BNA.npy'), conn_mat_rtoz)\n",
    "        print(' conn mat r to z')\n",
    "        \n",
    "    print(f' {time.time() - start_time} seconds!\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-appeal",
   "metadata": {},
   "source": [
    "# Make gradient Lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-comment",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo_lemon = pd.read_excel(join(main_path, 'LEMON_Demographic.xlsx'), sheet_name='n=212', skiprows=1, usecols='A:Z')\n",
    "sub_list_lemon = demo_lemon['ID']\n",
    "\n",
    "file_list = np.array([glob(join(main_path,'data_Lemon',i)) for i in sub_list_lemon]).flatten()\n",
    "\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-porcelain",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sparsity in [0.9]:\n",
    "\n",
    "#     sparsity = 0.75 # 0.75 0.5 0.8 0.7 0.6\n",
    "\n",
    "    k = str(int(100-sparsity*100))\n",
    "    k = k.zfill(2)\n",
    "\n",
    "    print(f'Top {k}')\n",
    "\n",
    "    folder_name = f'top{k}'\n",
    "\n",
    "    for i in file_list:\n",
    "        print(f'{np.where(file_list==i)[0].item()} / {len(file_list)}')\n",
    "\n",
    "        if not (os.path.isdir(os.path.join(i,folder_name))):\n",
    "            os.makedirs(os.path.join(i,folder_name))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        conn_mat = np.load(join(i,'conn_mat_pear_rtoz_BNA.npy'))[:210, :210]\n",
    "\n",
    "        noaff_conn_mat = gradient.compute_affinity(conn_mat, sparsity = sparsity)\n",
    "        aff_conn_mat = gradient.compute_affinity(conn_mat, kernel = 'cosine', sparsity = sparsity)\n",
    "\n",
    "        print(f'noaff isnan : {np.isnan(noaff_conn_mat).sum()}, symmetric : {gradient.is_symmetric(noaff_conn_mat)}')\n",
    "        print(f'aff isnan : {np.isnan(aff_conn_mat).sum()}, symmetric : {gradient.is_symmetric(aff_conn_mat)}')\n",
    "\n",
    "        np.save(join(i, folder_name, f'noaff_conn_mat_pear_top{k}_BNA_rmsubcor.npy'), noaff_conn_mat)\n",
    "        np.save(join(i, folder_name, f'aff_conn_mat_pear_top{k}_BNA_rmsubcor.npy'), aff_conn_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [10]:\n",
    "\n",
    "    Topk = str(i) # 25 50 20 30 40\n",
    "    folder = f'top{Topk}'\n",
    "\n",
    "    grp_noaff = np.load(join(save_path, f'noaff_grp_conn_mat_pear_top{Topk}_BNA_rmsubcor.npy'))\n",
    "    grp_aff = np.load(join(save_path, f'aff_grp_conn_mat_pear_top{Topk}_BNA_rmsubcor.npy'))\n",
    "\n",
    "    comp_num = 10\n",
    "\n",
    "    # PCA\n",
    "    print('PCA')\n",
    "    emb_pca = gradient.embedding.PCAMaps(n_components = comp_num)\n",
    "    emb_pca.fit(grp_noaff)\n",
    "    ref_lam_pca = emb_pca.lambdas_ \n",
    "    ref_PC_pca = emb_pca.maps_ \n",
    "\n",
    "    for i in file_list:\n",
    "        print(f'{np.where(file_list==i)[0].item()}', '', end='', flush=True)\n",
    "        dat = np.load(join(i,folder,f'noaff_conn_mat_pear_top{Topk}_BNA_rmsubcor.npy'))\n",
    "\n",
    "        emb_pca.fit(dat)\n",
    "        lam, grad = [None]*1, [None]*1\n",
    "        lam[0], grad[0] = emb_pca.lambdas_ , emb_pca.maps_\n",
    "\n",
    "        pa = gradient.ProcrustesAlignment(n_iter=10)\n",
    "        pa.fit(grad, reference=ref_PC_pca)\n",
    "        aligned = np.array(pa.aligned_)\n",
    "\n",
    "        np.save(join(i,folder,f'gradients_PCA_top{Topk}_BNA_rmsubcor.npy'), aligned)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    # Diffusion map\n",
    "    print('Diffusion Map')\n",
    "    emb_dm = gradient.embedding.DiffusionMaps(n_components = comp_num)\n",
    "    emb_dm.fit(grp_aff)\n",
    "    ref_lam_dm = emb_dm.lambdas_ \n",
    "    ref_PC_dm = emb_dm.maps_ \n",
    "\n",
    "    for i in file_list:\n",
    "        print(f'{np.where(file_list==i)[0].item()}', '', end='', flush=True)\n",
    "        dat = np.load(join(i,folder,f'aff_conn_mat_pear_top{Topk}_BNA_rmsubcor.npy'))\n",
    "\n",
    "        emb_dm.fit(dat)\n",
    "        lam, grad = [None]*1, [None]*1\n",
    "        lam[0], grad[0] = emb_dm.lambdas_ , emb_dm.maps_\n",
    "\n",
    "        pa = gradient.ProcrustesAlignment(n_iter=10)\n",
    "        pa.fit(grad, reference=ref_PC_dm)\n",
    "        aligned = np.array(pa.aligned_)\n",
    "\n",
    "        np.save(join(i,folder,f'gradients_DM_top{Topk}_BNA_rmsubcor.npy'), aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-panic",
   "metadata": {},
   "source": [
    "# Make sctx mean included FC eNkI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-dispatch",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo = pd.read_excel(join(main_path, 'Enhanced_NKI_sorted.xlsx'), sheet_name='n=424', skiprows=1)\n",
    "sub_list = demo['ID']\n",
    "age = demo['Age']\n",
    "sex = demo['Sex (M1F2)']\n",
    "\n",
    "file_list = []\n",
    "\n",
    "\n",
    "file_list = [join(main_path,'data',i) for i in sub_list]\n",
    "\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-delight",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make mean sctx FC file\n",
    "\n",
    "for i in file_list:\n",
    "    print(f'{file_list.index(i)+1} / {len(file_list)}')\n",
    "    \n",
    "    ts = np.load(join(i,'ts_BNA.npy'))\n",
    "    ts_ctx = ts[:210]\n",
    "    \n",
    "    # sctx list\n",
    "    ts_amyg = ts[210:214].mean(axis=0)\n",
    "    ts_hippo = ts[214:218].mean(axis=0)\n",
    "    ts_cau = ts[[218,219,226,227]].mean(axis=0)\n",
    "    ts_palli = ts[220:222].mean(axis=0)\n",
    "    ts_accum = ts[222:224].mean(axis=0)\n",
    "    ts_puta = ts[[224,225,228,229]].mean(axis=0)\n",
    "    ts_thal = ts[230:].mean(axis=0)\n",
    "    \n",
    "    ts_mean_sctx = np.vstack((ts_amyg, ts_hippo, ts_cau, ts_palli, ts_accum, ts_puta, ts_thal))\n",
    "    \n",
    "    ts_whole_Msctx = np.concatenate((ts_ctx, ts_mean_sctx), axis=0)\n",
    "    \n",
    "    np.save(join(i, 'ts_whole_Msctx_BNA.npy'), ts_whole_Msctx)\n",
    "    print('save ts')\n",
    "    \n",
    "    n_roi=217 # ctx 210 + sctx 7\n",
    "    conn_mat =np.nan_to_num(np.where(np.eye(n_roi) ==1, 0, np.corrcoef(ts_whole_Msctx)))\n",
    "    conn_mat_rtoz =  np.nan_to_num(np.arctanh(conn_mat),nan=0.0)\n",
    "    \n",
    "    np.save(join(i,'conn_mat_pear_whole_Msctx_BNA.npy'), conn_mat)\n",
    "    print('save conn mat')\n",
    "    np.save(join(i,'conn_mat_pear_rtoz_whole_Msctx_BNA.npy'), conn_mat_rtoz)\n",
    "    print('save conn mat r to z')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-violence",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sctx to ctx connection DC\n",
    "\n",
    "sparsity=0.9\n",
    "k = str(int(100-sparsity*100))\n",
    "k = k.zfill(2)\n",
    "print(f'Top {k}')\n",
    "folder_name = f'top{k}'\n",
    "\n",
    "for i in file_list:\n",
    "    print(f'{file_list.index(i)+1} / {len(file_list)}')\n",
    "    conn_mat_rtoz = np.load(join(i, 'conn_mat_pear_rtoz_whole_Msctx_BNA.npy'))[:,:210]\n",
    "#     print(conn_mat_rtoz.shape)\n",
    "    \n",
    "    noaff_conn_mat = gradient.compute_affinity(np.nan_to_num(conn_mat_rtoz, nan=0.0), sparsity=sparsity)\n",
    "    DC = noaff_conn_mat.sum(axis=1) # row-wise sum\n",
    "    np.save(join(i, folder_name, f'noaff_conn_mat_pear_top{k}_Msctx2ctx_BNA.npy'),noaff_conn_mat)\n",
    "    np.save(join(i, folder_name, f'DC_BNA_Msctx2ctx.npy'), DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "DC_file = np.array([np.load(join(i, 'top10', f'DC_BNA_Msctx2ctx.npy')) for i in file_list])\n",
    "print(DC_file.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC age, sex regressed out\n",
    "\n",
    "import statsmodels as sm\n",
    "from statsmodels.api import GLM\n",
    "\n",
    "list_reg_out = []\n",
    "\n",
    "for i in range(DC_file.shape[1]):\n",
    "    print(i, ' ', end='', flush=True)\n",
    "    a = GLM(DC_file[:,i],sm.api.add_constant(np.array([sex,age,sex*age]).T))\n",
    "    \n",
    "    res_a = a.fit()\n",
    "    # res.summary()\n",
    "\n",
    "    list_reg_out.append(res_a.resid_response)\n",
    "     \n",
    "DC_regout = np.array(list_reg_out).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-pastor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,x in enumerate(file_list):\n",
    "    print(f'{i+1} / {len(file_list)}')\n",
    "    np.save(join(x, 'top10', f'DC_BNA_Msctx2ctx_regout.npy'), DC_regout[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-forestry",
   "metadata": {},
   "source": [
    "# Make gradient eNKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-insider",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "demo = pd.read_excel(join(main_path, 'Enhanced_NKI_sorted.xlsx'), sheet_name='n=424', skiprows=1)\n",
    "sub_list = demo['ID']\n",
    "\n",
    "file_list = []\n",
    "\n",
    "\n",
    "file_list = [join(main_path,'data',i) for i in sub_list]\n",
    "\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-charge",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FC thrsholding\n",
    "\n",
    "for sparsity in [0.5, 0.8, 0.7, 0.6]:\n",
    "\n",
    "#     sparsity = 0.75 # 0.75 0.5 0.8 0.7 0.6\n",
    "\n",
    "    k = str(int(100-sparsity*100))\n",
    "    k = k.zfill(2)\n",
    "\n",
    "    print(f'Top {k}')\n",
    "\n",
    "    folder_name = f'top{k}'\n",
    "\n",
    "    for i in file_list:\n",
    "        print(f'{file_list.index(i)} / {len(file_list)}')\n",
    "\n",
    "        if not (os.path.isdir(os.path.join(i,f'{folder_name}'))):\n",
    "            os.makedirs(os.path.join(i,f'{folder_name}'))\n",
    "\n",
    "        conn_mat = np.load(join(i,'conn_mat_pear_rtoz_BNA.npy'))[:210, :210]\n",
    "\n",
    "        noaff_conn_mat = gradient.compute_affinity(conn_mat, sparsity = sparsity)\n",
    "        aff_conn_mat = gradient.compute_affinity(conn_mat, kernel = 'cosine', sparsity = sparsity)\n",
    "\n",
    "        print(f'noaff isnan : {np.isnan(noaff_conn_mat).sum()}, symmetric : {gradient.is_symmetric(noaff_conn_mat)}')\n",
    "        print(f'aff isnan : {np.isnan(aff_conn_mat).sum()}, symmetric : {gradient.is_symmetric(aff_conn_mat)}')\n",
    "\n",
    "        np.save(join(i, folder_name, f'noaff_conn_mat_pear_top{k}_BNA_rmsubcor.npy'), noaff_conn_mat)\n",
    "        np.save(join(i, folder_name, f'aff_conn_mat_pear_top{k}_BNA_rmsubcor.npy'), aff_conn_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-wrist",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make eigenvector\n",
    "\n",
    "for i in [50, 20, 30, 40]:\n",
    "\n",
    "    Topk = str(i) # 25 50 20 30 40\n",
    "    folder = f'top{Topk}'\n",
    "\n",
    "    grp_noaff = np.load(join(save_path, f'noaff_grp_conn_mat_pear_top{Topk}_BNA_rmsubcor.npy'))\n",
    "    grp_aff = np.load(join(save_path, f'aff_grp_conn_mat_pear_top{Topk}_BNA_rmsubcor.npy'))\n",
    "\n",
    "    comp_num = 10\n",
    "\n",
    "    # PCA\n",
    "    print('PCA')\n",
    "    emb_pca = gradient.embedding.PCAMaps(n_components = comp_num)\n",
    "    emb_pca.fit(grp_noaff)\n",
    "    ref_lam_pca = emb_pca.lambdas_ \n",
    "    ref_PC_pca = emb_pca.maps_ \n",
    "\n",
    "    for i in file_list:\n",
    "        print(f'{file_list.index(i)}', '', end='', flush=True)\n",
    "        dat = np.load(join(i,folder,f'noaff_conn_mat_pear_top{Topk}_BNA_rmsubcor.npy'))\n",
    "\n",
    "        emb_pca.fit(dat)\n",
    "        lam, grad = [None]*1, [None]*1\n",
    "        lam[0], grad[0] = emb_pca.lambdas_ , emb_pca.maps_\n",
    "\n",
    "        pa = gradient.ProcrustesAlignment(n_iter=10)\n",
    "        pa.fit(grad, reference=ref_PC_pca)\n",
    "        aligned = np.array(pa.aligned_)\n",
    "\n",
    "        np.save(join(i,folder,f'gradients_PCA_top{Topk}_BNA_rmsubcor.npy'), aligned)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    # Diffusion map\n",
    "    print('Diffusion Map')\n",
    "    emb_dm = gradient.embedding.DiffusionMaps(n_components = comp_num)\n",
    "    emb_dm.fit(grp_aff)\n",
    "    ref_lam_dm = emb_dm.lambdas_ \n",
    "    ref_PC_dm = emb_dm.maps_ \n",
    "\n",
    "    for i in file_list:\n",
    "        print(f'{file_list.index(i)}', '', end='', flush=True)\n",
    "        dat = np.load(join(i,folder,f'aff_conn_mat_pear_top{Topk}_BNA_rmsubcor.npy'))\n",
    "\n",
    "        emb_dm.fit(dat)\n",
    "        lam, grad = [None]*1, [None]*1\n",
    "        lam[0], grad[0] = emb_dm.lambdas_ , emb_dm.maps_\n",
    "\n",
    "        pa = gradient.ProcrustesAlignment(n_iter=10)\n",
    "        pa.fit(grad, reference=ref_PC_dm)\n",
    "        aligned = np.array(pa.aligned_)\n",
    "\n",
    "        np.save(join(i,folder,f'gradients_DM_top{Topk}_BNA_rmsubcor.npy'), aligned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_aff = np.load(join(path_subj, f'aff_grp_conn_mat_pear_top10_BNA_rmsubcor.npy'))\n",
    "\n",
    "comp_num = 10\n",
    "\n",
    "emb_dm = gradient.embedding.DiffusionMaps(n_components = comp_num)\n",
    "emb_dm.fit(grp_aff)\n",
    "ref_lam_dm = emb_dm.lambdas_ \n",
    "ref_PC_dm = emb_dm.maps_ \n",
    "\n",
    "sns.barplot(x = list(range(1,11) ), y = ref_lam_dm/ref_lam_dm.sum(), palette='Greys_r') # 'ch:2,r=1,l=.6' Greys_r\n",
    "plt.xlabel('Principal component', fontsize = 15)\n",
    "plt.ylabel('The ratio of variance', fontsize = 15)\n",
    "plt.title('Explained Variance')\n",
    "plt.grid(False)\n",
    "\n",
    "print(ref_lam_dm/ref_lam_dm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-kingdom",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-mauritius",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, OneHotEncoder\n",
    "\n",
    "age = demo['Age']\n",
    "sex = demo['Sex (M1F2)']\n",
    "BMI = demo['BMI']\n",
    "WHR = demo['WHR']\n",
    "EDE_Q_R = demo['Restraint']\n",
    "EDE_Q_E_con = demo['Eating concern']\n",
    "EDE_Q_S_con = demo['Shape concern']\n",
    "EDE_Q_W_con = demo['Weight concern']\n",
    "TFEQ_F1 = demo['Factor 1']\n",
    "TFEQ_F2 = demo['Factor 2']\n",
    "TFEQ_F3 = demo['Factor 3']\n",
    "\n",
    "EDE_Q_global = (EDE_Q_R+EDE_Q_E_con+EDE_Q_S_con+EDE_Q_W_con) / 4\n",
    "EDE_Q_con = (EDE_Q_E_con+EDE_Q_S_con+EDE_Q_W_con) / 3\n",
    "\n",
    "scaler = MinMaxScaler() # MinMaxScaler StandardScaler MaxAbsScaler RobustScaler\n",
    " \n",
    "y_target = np.array(BMI)\n",
    "y_scaled = scaler.fit_transform(np.array(y_target).reshape(-1,1)).reshape(-1)\n",
    "\n",
    "demographic_label = ['age', 'BMI', 'WHR', 'EDE_Q_R', 'EDE_Q_E_con', 'EDE_Q_S_con', 'EDE_Q_W_con', 'TFEQ_F1', 'TFEQ_F2', 'TFEQ_F3']\n",
    "\n",
    "for i, x in enumerate([age, BMI, WHR, EDE_Q_R, EDE_Q_E_con, EDE_Q_S_con, EDE_Q_W_con, TFEQ_F1, TFEQ_F2, TFEQ_F3]):\n",
    "    print(demographic_label[i], ':', np.array(x).shape, np.round(x.mean(),2), np.round(x.std(),2), '      Min Max', np.round(x.min(),2), np.round(x.max(),2))\n",
    "    \n",
    "print('Sex F M : ',len(np.where(sex==2)[0]), len(np.where(sex==1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.where((BMI>=18.5)&(BMI<25))[0]))\n",
    "print(len(np.where((BMI>=25)&(BMI<30))[0]))\n",
    "print(len(np.where(BMI>=30)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-boston",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, OneHotEncoder\n",
    "# Lemon\n",
    "\n",
    "age_lemon = demo_lemon['Age']\n",
    "sex_lemon = demo_lemon['Gender_ 1=male_2=female']\n",
    "BMI_lemon = demo_lemon['BMI (kg/m2)']\n",
    "TFEQ_F1_lemon = demo_lemon['FEV_KK']\n",
    "TFEQ_F2_lemon = demo_lemon['FEV_STOER']\n",
    "TFEQ_F3_lemon = demo_lemon['FEV_HUNGER']\n",
    "\n",
    "scaler = MinMaxScaler() # MinMaxScaler StandardScaler MaxAbsScaler RobustScaler\n",
    " \n",
    "y_target = np.array(BMI_lemon)\n",
    "y_scaled = scaler.fit_transform(np.array(y_target).reshape(-1,1)).reshape(-1)\n",
    "    \n",
    "demographic_label = ['age', 'BMI', 'TFEQ_F1', 'TFEQ_F2', 'TFEQ_F3']\n",
    "\n",
    "for i, x in enumerate([age_lemon, BMI_lemon, TFEQ_F1_lemon, TFEQ_F2_lemon, TFEQ_F3_lemon]):\n",
    "    print(demographic_label[i], ':', np.array(x).shape, np.round(x.mean(),2), np.round(x.std(),2), '      Min Max', np.round(x.min(),2), np.round(x.max(),2))\n",
    "    \n",
    "print('Sex F M : ',len(np.where(sex_lemon==2)[0]), len(np.where(sex_lemon==1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.where((BMI_lemon>=18.5)&(BMI_lemon<25))[0]))\n",
    "print(len(np.where((BMI_lemon>=25)&(BMI_lemon<30))[0]))\n",
    "print(len(np.where(BMI_lemon>=30)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-perception",
   "metadata": {},
   "source": [
    "# Demographinc statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dis_demo = [age, BMI, TFEQ_F1, TFEQ_F2, TFEQ_F3]\n",
    "Rep_demo = [age_lemon, BMI_lemon, TFEQ_F1_lemon, TFEQ_F2_lemon, TFEQ_F3_lemon]\n",
    "demographic_label = ['age', 'BMI', 'TFEQ_F1', 'TFEQ_F2', 'TFEQ_F3']\n",
    "\n",
    "for i in range(len(Dis_demo)):\n",
    "\n",
    "    [s_0,p_0] = sc.stats.ttest_ind(Dis_demo[i], Rep_demo[i], equal_var = False, axis=0)\n",
    "    print(f'{demographic_label[i]} : {p_0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discovery, replication dataset sex chi-square\n",
    "\n",
    "chi_ob = [282, 142] # discovery(eNKI) F:M\n",
    "chi_expect = [75,137] # replication(Lemon) F:M\n",
    "\n",
    "chi_stat, chi_pval = sc.stats.chisquare(chi_ob, f_exp=chi_expect, axis=None, ddof = [0,1,2])\n",
    "\n",
    "print(chi_stat)\n",
    "print(chi_pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = join(main_path,'data')\n",
    "path_data_Lemon = join(main_path,'data_Lemon')\n",
    "\n",
    "topk = 'top10' # top10 top20 top25 top30 top40 top50\n",
    "\n",
    "file_list = np.concatenate([np.load(join(path_data,i, topk, f'gradients_DM_{topk}_BNA_rmsubcor.npy')) for i in sub_list], axis=0) # gradients_PCA_top10_BNA gradients_DM_top10_BNA\n",
    "# file_list = np.concatenate([np.load(join(path_data_Lemon,i, topk, f'gradients_DM_{topk}_BNA_rmsubcor.npy')) for i in sub_list_lemon], axis=0) # gradients_PCA_top10_BNA gradients_DM_top10_BNA\n",
    "\n",
    "\n",
    "grad1_list = file_list[:,:,0]\n",
    "grad2_list = file_list[:,:,1]\n",
    "grad3_list = file_list[:,:,2]\n",
    "\n",
    "# gradient z normalized\n",
    "\n",
    "grad1_list_znorm = sc.stats.zscore(grad1_list, axis=1)\n",
    "grad2_list_znorm = sc.stats.zscore(grad2_list, axis=1)\n",
    "grad3_list_znorm = sc.stats.zscore(grad3_list, axis=1)\n",
    "\n",
    "# gradient concatenate\n",
    "grad_list_znorm_stacked = np.concatenate((grad1_list_znorm, grad2_list_znorm, grad3_list_znorm), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-hostel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gradient age, sex regressed out\n",
    "\n",
    "import statsmodels as sm\n",
    "from statsmodels.api import GLM\n",
    "\n",
    "list_reg_out_a = []\n",
    "list_reg_out_b = []\n",
    "list_reg_out_c = []\n",
    "list_reg_out_d = []\n",
    "\n",
    "for i in range(grad_list_znorm_stacked.shape[1]):\n",
    "    print(i, ' ', end='', flush=True)\n",
    "    a = GLM(grad_list_znorm_stacked[:,i],sm.api.add_constant(np.array([sex,age,sex*age]).T))\n",
    "    \n",
    "    res_a = a.fit()\n",
    "    # res.summary()\n",
    "\n",
    "    list_reg_out_a.append(res_a.resid_response)\n",
    "     \n",
    "grad_list_znorm_stacked_regout = np.array(list_reg_out_a).T\n",
    "\n",
    "\n",
    "for i in range(grad1_list_znorm.shape[1]):\n",
    "    print(i, ' ', end='', flush=True)\n",
    "    b = GLM(grad1_list_znorm[:,i],sm.api.add_constant(np.array([sex,age,sex*age]).T))\n",
    "    c = GLM(grad2_list_znorm[:,i],sm.api.add_constant(np.array([sex,age,sex*age]).T))\n",
    "    d = GLM(grad3_list_znorm[:,i],sm.api.add_constant(np.array([sex,age,sex*age]).T))\n",
    "\n",
    "    res_b = b.fit()\n",
    "    res_c = c.fit()\n",
    "    res_d = d.fit()\n",
    "    # res.summary()\n",
    "\n",
    "    list_reg_out_b.append(res_b.resid_response)\n",
    "    list_reg_out_c.append(res_c.resid_response)\n",
    "    list_reg_out_d.append(res_d.resid_response)\n",
    "    \n",
    "    \n",
    "grad1_list_znorm_regout = np.array(list_reg_out_b).T\n",
    "grad2_list_znorm_regout = np.array(list_reg_out_c).T\n",
    "grad3_list_znorm_regout = np.array(list_reg_out_d).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-hampshire",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad1_DM_temp = np.load(join(main_path, 'data_HCP', 'grad1_DM_top10_temp_BNA_rmsubcor.npy'))[:210]\n",
    "grad2_DM_temp = np.load(join(main_path, 'data_HCP', 'grad2_DM_top10_temp_BNA_rmsubcor.npy'))[:210]\n",
    "grad3_DM_temp = np.load(join(main_path, 'data_HCP', 'grad3_DM_top10_temp_BNA_rmsubcor.npy'))[:210]\n",
    "grad_DM_temp = np.array([grad1_DM_temp, grad2_DM_temp, grad3_DM_temp]).T\n",
    "\n",
    "grad_list_znorm_stacked_regout_3d = np.dstack((grad1_list_znorm_regout, grad2_list_znorm_regout, grad3_list_znorm_regout))\n",
    "\n",
    "# Manifold Eccentricity\n",
    "ncompo = 3\n",
    "\n",
    "nroi = grad_DM_temp.shape[0]\n",
    "gm_center = grad_DM_temp[:,:ncompo].mean()\n",
    "ME = np.array([np.sqrt(np.sum(np.square(gm_center-file_list[i,:,:ncompo]), axis=1)) for i in range(len(file_list))])\n",
    "ME_regout = np.array([np.sqrt(np.sum(np.square(gm_center-grad_list_znorm_stacked_regout_3d[i,:,:ncompo]), axis=1)) for i in range(len(grad_list_znorm_stacked_regout_3d))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
