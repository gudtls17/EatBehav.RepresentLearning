{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prostate-center",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "\n",
    "print(f\"Load model : {load_model}\")\n",
    "\n",
    "if load_model:\n",
    "\n",
    "    trial_test = '01'\n",
    "    epoch_test = 499\n",
    "\n",
    "    save_path = join(main_path,'result/latent_210/main_result/AE')\n",
    "    load_test_model = join(save_path, trial_test + '_best_AE_epoch%d.pkl' % epoch_test)\n",
    "    \n",
    "    model_result = AutoEncoder().to(device) # AutoEncoder LinearVAE\n",
    "    model_result.load_state_dict(torch.load(load_test_model), strict=False) # best_model torch.load(load_test_model)\n",
    "\n",
    "# print(load_test_model)\n",
    "\n",
    "else:\n",
    "    model_result = AutoEncoder().to(device) # AutoEncoder LinearVAE\n",
    "    model_result.load_state_dict(best_model.state_dict(), strict=False) # best_model torch.load(load_test_model)\n",
    "\n",
    "# test_loss, latent, x1, x1_hat, recon_loss = test(model_result, test_loader, layer='AE3')\n",
    "\n",
    "test_loss, x_test, x_hat_test, _, latent_test = validate(model_result, test_loader, layer='AE') # test_loader lemon_loader\n",
    "\n",
    "print(f'test loss: {test_loss:.4f}')\n",
    "\n",
    "test_result = torch.cat(latent_test).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(join(main_path,'result/latent_210/Ablation','top40','test_latent.npy'), test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-gathering",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad_hat_testset = torch.cat(x_hat_test).detach().cpu().numpy()\n",
    "np.save(join(main_path,'result/latent_210/main_result/AE','grad_hat_testset.npy'), grad_hat_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-tribe",
   "metadata": {},
   "source": [
    "# Reconstruction evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat(x_test).detach().cpu().numpy()\n",
    "y = torch.cat(x_hat_test).detach().cpu().numpy()\n",
    "\n",
    "corr_test = np.array([sc.stats.pearsonr(x[i],y[i])[0] for i in range(len(x))])\n",
    "p_test = np.array([sc.stats.pearsonr(x[i],y[i])[1] for i in range(len(x))])\n",
    "\n",
    "print(corr_test.mean())\n",
    "print(np.median(corr_test))\n",
    "print(corr_test.max())\n",
    "print(corr_test.min())\n",
    "print(corr_test.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-sacramento",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "test_corr_a = np.median(corr_test) + corr_test.std(axis=0)\n",
    "test_corr_b = np.median(corr_test) - corr_test.std(axis=0)\n",
    "\n",
    "median_idx = np.where(corr_test==np.median(corr_test))[0]\n",
    "\n",
    "x_pred = np.linspace(x[median_idx].min(),x[median_idx].max(),50)\n",
    "x_pred2 = add_constant(x_pred)\n",
    "y_pred = x_pred*np.median(corr_test)*np.median(corr_test)\n",
    "upper = x_pred*test_corr_a*test_corr_a \n",
    "lower = x_pred*test_corr_b*test_corr_b \n",
    "\n",
    "plt.figure(1, figsize=(6,6))\n",
    "plt.plot(x_pred,y_pred, linewidth=4, color='k')\n",
    "plt.fill_between(x_pred, lower, upper,color='#888888', alpha=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-1.1,1.1)\n",
    "plt.xticks([-1,0,1], fontsize=10)\n",
    "plt.ylim(-1.1,1.1)\n",
    "plt.yticks([-1,0,1], fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-welcome",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_latent_test = result\n",
    "final_latent_test\n",
    "\n",
    "plt.figure(1)\n",
    "plt.matshow(final_latent_test)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.matshow(final_latent_test[np.where(label_manual==0)[0]])\n",
    "plt.clim(-1,1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(3)\n",
    "plt.matshow(final_latent_test[np.where(label_manual==1)[0]])\n",
    "plt.clim(-1,1)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(4)\n",
    "plt.matshow(final_latent_test[np.where(label_manual==2)[0]])\n",
    "plt.clim(-1,1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-ballet",
   "metadata": {},
   "source": [
    "# Consensus clustering definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-earthquake",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import bisect\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ConsensusCluster:\n",
    "    \"\"\"\n",
    "      Implementation of Consensus clustering, following the paper\n",
    "      https://link.springer.com/content/pdf/10.1023%2FA%3A1023949509487.pdf\n",
    "      Args:\n",
    "        * cluster -> clustering class\n",
    "        * NOTE: the class is to be instantiated with parameter `n_clusters`,\n",
    "          and possess a `fit_predict` method, which is invoked on data.\n",
    "        * L -> smallest number of clusters to try\n",
    "        * K -> biggest number of clusters to try\n",
    "        * H -> number of resamplings for each cluster number\n",
    "        * resample_proportion -> percentage to sample\n",
    "        * Mk -> consensus matrices for each k (shape =(K,data.shape[0],data.shape[0]))\n",
    "                (NOTE: every consensus matrix is retained, like specified in the paper)\n",
    "        * Ak -> area under CDF for each number of clusters \n",
    "                (see paper: section 3.3.1. Consensus distribution.)\n",
    "        * deltaK -> changes in areas under CDF\n",
    "                (see paper: section 3.3.1. Consensus distribution.)\n",
    "        * self.bestK -> number of clusters that was found to be best\n",
    "      \"\"\"\n",
    "\n",
    "    def __init__(self, cluster, L, K, H, resample_proportion=0.5):\n",
    "        assert 0 <= resample_proportion <= 1, \"proportion has to be between 0 and 1\"\n",
    "        self.cluster_ = cluster\n",
    "        self.resample_proportion_ = resample_proportion\n",
    "        self.L_ = L\n",
    "        self.K_ = K\n",
    "        self.H_ = H\n",
    "        self.Mk = None\n",
    "        self.Ak = None\n",
    "        self.deltaK = None\n",
    "        self.bestK = None\n",
    "\n",
    "    def _internal_resample(self, data, proportion):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          * data -> (examples,attributes) format\n",
    "          * proportion -> percentage to sample\n",
    "        \"\"\"\n",
    "        resampled_indices = np.random.choice(\n",
    "            range(data.shape[0]), size=int(data.shape[0]*proportion), replace=False)\n",
    "        return resampled_indices, data[resampled_indices, :]\n",
    "\n",
    "    def fit(self, data, verbose=False):\n",
    "        \"\"\"\n",
    "        Fits a consensus matrix for each number of clusters\n",
    "\n",
    "        Args:\n",
    "          * data -> (examples,attributes) format\n",
    "          * verbose -> should print or not\n",
    "        \"\"\"\n",
    "        Mk = np.zeros((self.K_-self.L_, data.shape[0], data.shape[0]))\n",
    "        Is = np.zeros((data.shape[0],)*2)\n",
    "        plot_Ak = []\n",
    "        for k in range(self.L_, self.K_):  # for each number of clusters\n",
    "            i_ = k-self.L_\n",
    "            if verbose:\n",
    "                print(\"At k = %d, aka. iteration = %d\" % (k, i_))\n",
    "            for h in range(self.H_):  # resample H times\n",
    "                if verbose:\n",
    "                    print(\"\\tAt resampling h = %d, (k = %d)\" % (h, k))\n",
    "                resampled_indices, resample_data = self._internal_resample(\n",
    "                    data, self.resample_proportion_)\n",
    "                Mh = self.cluster_(n_clusters=k).fit_predict(resample_data)\n",
    "                # find indexes of elements from same clusters with bisection\n",
    "                # on sorted array => this is more efficient than brute force search\n",
    "                id_clusts = np.argsort(Mh)\n",
    "                sorted_ = Mh[id_clusts]\n",
    "                for i in range(k):  # for each cluster\n",
    "                    ia = bisect.bisect_left(sorted_, i)\n",
    "                    ib = bisect.bisect_right(sorted_, i)\n",
    "                    is_ = id_clusts[ia:ib]\n",
    "                    ids_ = np.array(list(combinations(is_, 2))).T\n",
    "                    # sometimes only one element is in a cluster (no combinations)\n",
    "                    if ids_.size != 0:\n",
    "                        Mk[i_, ids_[0], ids_[1]] += 1\n",
    "                # increment counts\n",
    "                ids_2 = np.array(list(combinations(resampled_indices, 2))).T\n",
    "                Is[ids_2[0], ids_2[1]] += 1\n",
    "            Mk[i_] /= Is+1e-8  # consensus matrix\n",
    "            # Mk[i_] is upper triangular (with zeros on diagonal), we now make it symmetric\n",
    "            Mk[i_] += Mk[i_].T\n",
    "            Mk[i_, range(data.shape[0]), range(\n",
    "                data.shape[0])] = 1  # always with self\n",
    "            Is.fill(0)  # reset counter\n",
    "        self.Mk = Mk\n",
    "        # fits areas under the CDFs\n",
    "        self.Ak = np.zeros(self.K_-self.L_)\n",
    "        for i, m in enumerate(Mk):\n",
    "            hist, bins = np.histogram(m.ravel(), density=True)\n",
    "            self.Ak[i] = np.sum(h*(b-a)\n",
    "                             for b, a, h in zip(bins[1:], bins[:-1], np.cumsum(hist)))\n",
    "            \n",
    "            for b, a, h in zip(bins[1:], bins[:-1], np.cumsum(hist)):\n",
    "                plot_Ak.append(h*(b-a))\n",
    "#             print(plot_Ak)\n",
    "            \n",
    "        # fits differences between areas under CDFs\n",
    "        self.deltaK = np.array([(Ab-Aa)/Aa if i >= 2 else Aa\n",
    "                                for Ab, Aa, i in zip(self.Ak[1:], self.Ak[:-1], range(self.L_, self.K_-1))])\n",
    "        self.bestK = np.argmax(self.deltaK) + \\\n",
    "            self.L_ if self.deltaK.size > 0 else self.L_\n",
    "        print(f'AK = {self.Ak}')\n",
    "        print(f'deltaK = {self.deltaK}')\n",
    "        \n",
    "        return self.Mk, self.Ak, self.deltaK\n",
    "       \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predicts on the consensus matrix, for best found cluster number\n",
    "        \"\"\"\n",
    "        assert self.Mk is not None, \"First run fit\"\n",
    "        return self.cluster_(n_clusters=self.bestK).fit_predict(\n",
    "            1-self.Mk[self.bestK-self.L_])\n",
    "\n",
    "    def predict_data(self, data):\n",
    "        \"\"\"\n",
    "        Predicts on the data, for best found cluster number\n",
    "        Args:\n",
    "          * data -> (examples,attributes) format \n",
    "        \"\"\"\n",
    "        assert self.Mk is not None, \"First run fit\"\n",
    "        return self.cluster_(n_clusters=self.bestK).fit_predict(\n",
    "            data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-psychology",
   "metadata": {},
   "source": [
    "# Consensus clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
    "\n",
    "cc = ConsensusCluster(KMeans, 2, 9, 100, resample_proportion=0.9) # KMeans # AgglomerativeClustering # SpectralClustering\n",
    "\n",
    "Mk, Ak, deltak = cc.fit(result) # train_result valid_result test_result\n",
    "\n",
    "cluster_labels = cc.predict_data(result) # train_result valid_result test_result\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-jewel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# latent 210 result Optimal K\n",
    "\n",
    "deltak = [-0.11931122,  0.10498302,  0.0881342,   0.02758437,  0.09317429,  0.04796325]\n",
    "\n",
    "sns.set(style = 'whitegrid', font_scale=2.5)\n",
    "plt.figure(1, (7,5))\n",
    "plt.plot([2,3,4,5,6,7],[i if i >0 else 0 for i in deltak], '-o', color='k')\n",
    "plt.ylim([-0.01,0.1])\n",
    "plt.xticks([2,3,4,5,6,7])\n",
    "plt.yticks([0,0.02,0.04,0.06,0.08,0.1, 0.12])\n",
    "plt.xlabel('Number of clusters K')\n",
    "plt.ylabel('Consensus coefficient', fontsize=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-rolling",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "clusterer = KMeans(n_clusters=3) # KMeans AgglomerativeClustering\n",
    "# clusterer = GaussianMixture(n_components=3, random_state=None) # GaussianMixture\n",
    "cluster_label = clusterer.fit_predict(result) # train_result valid_result test_result \n",
    "# cluster_label\n",
    "label_idx = label_subj(cluster_label)\n",
    "\n",
    "sub1 = label_idx[0]\n",
    "sub2 = label_idx[1]\n",
    "sub3 = label_idx[2]\n",
    "\n",
    "print(sub1, '\\n',sub2, '\\n',sub3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 210 Main cluster label\n",
    "cluster_label_AE = np.array([2, 0, 1, 2, 2, 1, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 2, 2, 0,\n",
    "       0, 1, 1, 0, 1, 0, 2, 0, 2, 2, 1, 0, 0, 2, 2, 2, 0, 0, 2, 1, 2, 1,\n",
    "       0, 1, 0, 0, 0, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0, 1, 0, 2,\n",
    "       2, 2, 2, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 2, 0, 1, 2, 2, 1])\n",
    "\n",
    "sub_idx = label_subj(cluster_label_AE)\n",
    "\n",
    "sub1_AE = sub_idx[0]\n",
    "sub2_AE = sub_idx[1]\n",
    "sub3_AE = sub_idx[2]\n",
    "\n",
    "print(sub1_AE, '\\n',sub2_AE, '\\n',sub3_AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-gender",
   "metadata": {},
   "source": [
    "# Plot one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "y = TFEQ_F1 # BMI WHR EDE_Q_R EDE_Q_con EDE_Q_E_con EDE_Q_S_con EDE_Q_W_con TFEQ_F1 TFEQ_F2 TFEQ_F3 \n",
    "\n",
    "idx = otest_idx # itrain_idx itest_idx otest_idx\n",
    "\n",
    "df = pd.DataFrame([np.array(y), cluster_label_lemon+1]) # cluster_labels  label_manual\n",
    "df = df.T\n",
    "df.columns = ['Score','Subtype']\n",
    "\n",
    "plt.figure(1,(7,7))\n",
    "sns.set(style = 'whitegrid', font_scale=2.5)\n",
    "sns.boxplot(x = df.columns[1] , y = df.columns[0], data = df, palette = 'gist_yarg', width=0.5)\n",
    "sns.swarmplot(x = df.columns[1] , y = df.columns[0], data = df, color = 'k', size = 4)\n",
    "\n",
    "plt.ylim([-1,20])\n",
    "# plt.yticks([20,25,30,35,40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-execution",
   "metadata": {},
   "source": [
    "# Plot all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial = 0\n",
    "score_plot(trial, otest_idx, cluster_label_420+1, save=False, plot_close=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-scoop",
   "metadata": {},
   "source": [
    "# Statistical analysis (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial=1\n",
    "\n",
    "stat_result(trial, main_otest_idx, main_cluster_label, save=False, result_show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_score(cluster_label_AE, main_otest_idx) # cluster_label_gradient cluster_label_AE\n",
    "\n",
    "# subtype_score(cluster_label_lemon, np.arange(212)) # Lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "BMI_sub1_main = np.array(BMI)[main_otest_idx][sub1_main]\n",
    "BMI_sub2_main = np.array(BMI)[main_otest_idx][sub2_main]\n",
    "BMI_sub3_main = np.array(BMI)[main_otest_idx][sub3_main]\n",
    "\n",
    "TFEQ_F1_sub1_main = np.array(TFEQ_F1)[main_otest_idx][sub1_main]\n",
    "TFEQ_F1_sub2_main = np.array(TFEQ_F1)[main_otest_idx][sub2_main]\n",
    "TFEQ_F1_sub3_main = np.array(TFEQ_F1)[main_otest_idx][sub3_main]\n",
    "\n",
    "TFEQ_F2_sub1_main = np.array(TFEQ_F2)[main_otest_idx][sub1_main]\n",
    "TFEQ_F2_sub2_main = np.array(TFEQ_F2)[main_otest_idx][sub2_main]\n",
    "TFEQ_F2_sub3_main = np.array(TFEQ_F2)[main_otest_idx][sub3_main]\n",
    "\n",
    "TFEQ_F3_sub1_main = np.array(TFEQ_F3)[main_otest_idx][sub1_main]\n",
    "TFEQ_F3_sub2_main = np.array(TFEQ_F3)[main_otest_idx][sub2_main]\n",
    "TFEQ_F3_sub3_main = np.array(TFEQ_F3)[main_otest_idx][sub3_main]\n",
    "\n",
    "score_main = sc.io.savemat(join(main_path, 'result/latent_210/main_result/AE/score_main.mat'), {'BMI_sub1_main':BMI_sub1_main, 'BMI_sub2_main':BMI_sub2_main, 'BMI_sub3_main':BMI_sub3_main, 'TFEQ_F1_sub1_main':TFEQ_F1_sub1_main,\n",
    "                                                                                              'TFEQ_F1_sub2_main':TFEQ_F1_sub2_main, 'TFEQ_F1_sub3_main':TFEQ_F1_sub3_main, 'TFEQ_F2_sub1_main':TFEQ_F2_sub1_main, 'TFEQ_F2_sub2_main':TFEQ_F2_sub2_main,\n",
    "                                                                                              'TFEQ_F2_sub3_main':TFEQ_F2_sub3_main, 'TFEQ_F3_sub1_main':TFEQ_F3_sub1_main, 'TFEQ_F3_sub2_main':TFEQ_F3_sub2_main, 'TFEQ_F3_sub3_main':TFEQ_F3_sub3_main})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-server",
   "metadata": {},
   "source": [
    "# Score permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-oklahoma",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_label = cluster_label_lemon # main_cluster_label cluster_l\n",
    "random_state = 44             # main perm 2, 30% 10, 20% 0, Gau 15,AE_dropx 15, AE_420 15, AE_120 15, ME 16, gradient 16, boot_470 58, boot_562 6, boot_626 4, Lemon 44\n",
    "\n",
    "\n",
    "BMI_p = permutation_ttest(BMI_lemon, np.arange(212), c_label, random_state)\n",
    "TFEQ_F1_p = permutation_ttest(TFEQ_F1_lemon, np.arange(212), c_label, random_state)\n",
    "TFEQ_F2_p = permutation_ttest(TFEQ_F2_lemon, np.arange(212), c_label, random_state)\n",
    "TFEQ_F3_p = permutation_ttest(TFEQ_F3_lemon, np.arange(212), c_label, random_state) \n",
    "\n",
    "for i in [BMI_p, TFEQ_F1_p, TFEQ_F2_p, TFEQ_F3_p]:\n",
    "\n",
    "    p_fdr = sm.stats.multitest.multipletests(i, alpha=0.05,method='fdr_bh')\n",
    "    print(p_fdr[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-liquid",
   "metadata": {},
   "source": [
    "# Significant different latent element from each subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_otest_idx = np.load(join(main_path,'result/latent_210/main_result/AE/subj_idx.npy'), allow_pickle=True).item()['otest_idx']\n",
    "main_cluster_label = np.load(join(main_path,'result/latent_210/main_result/AE/AE_cluster_label_idx.npy'))\n",
    "main_latent = np.load(join(main_path,'result/latent_210/main_result/AE/latent.npy'))\n",
    "\n",
    "sub_idx = label_subj(main_cluster_label)\n",
    "\n",
    "sub1_main = sub_idx[0]\n",
    "sub2_main = sub_idx[1]\n",
    "sub3_main = sub_idx[2]\n",
    "\n",
    "main_latent_sub1 = main_latent[sub1_main]\n",
    "main_latent_sub2 = main_latent[sub2_main]\n",
    "main_latent_sub3 = main_latent[sub3_main]\n",
    "\n",
    "main_latent_sub12 = np.concatenate((main_latent_sub1,main_latent_sub2),axis=0)\n",
    "main_latent_sub13 = np.concatenate((main_latent_sub1,main_latent_sub3),axis=0)\n",
    "main_latent_sub23 = np.concatenate((main_latent_sub2,main_latent_sub3),axis=0)\n",
    "\n",
    "print('')\n",
    "sig_idx_sub1 = latent_sig_elem_ttest(main_latent_sub1, main_latent_sub23, 50)\n",
    "print('')\n",
    "sig_idx_sub2 = latent_sig_elem_ttest(main_latent_sub2, main_latent_sub13, 50)\n",
    "print('')\n",
    "sig_idx_sub3 = latent_sig_elem_ttest(main_latent_sub3, main_latent_sub12, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-policy",
   "metadata": {},
   "source": [
    "# Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-international",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def IG_grad(sub_main_idx, sig_idx):\n",
    "    X = torch.tensor(grad_list_znorm_stacked_regout[main_otest_idx][sub_main_idx]).float().to('cpu') # [subj_num, Multi-gradients]\n",
    "    X.requires_grad_()\n",
    "\n",
    "    IG_grad_elem = []\n",
    "    attr_list = []\n",
    "    IG_sum = np.zeros(grad_list_znorm_stacked_regout[main_otest_idx].shape)\n",
    "    for i in sig_idx: # sig_idx\n",
    "#         attr, delta = lrp.attribute(X,target=0, return_convergence_delta=True)\n",
    "        attr = nig.attribute(X, int(i))\n",
    "        attr = attr.detach().numpy()\n",
    "        IG_sum += attr\n",
    "    \n",
    "    return IG_sum.mean(axis=0), IG_sum # Finally, attr_list_sum = IG_sum/subj_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-controversy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, LRP, NeuronIntegratedGradients\n",
    "from captum.attr import LayerConductance\n",
    "from captum.attr import NeuronConductance\n",
    "\n",
    "load_test_model = join(main_path,'result/latent_210/main_result/AE','01_best_AE_epoch499.pkl')\n",
    "\n",
    "model = AutoEncoder_en().to('cpu')\n",
    "model.load_state_dict(torch.load(load_test_model), strict=False)\n",
    "\n",
    "# Model interpretation\n",
    "nig = NeuronIntegratedGradients(model, model.encoder2)\n",
    "\n",
    "IG_testmean, IG_testsum = IG_grad(np.arange(len(main_latent)), np.arange(210))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "IG1_testmean = IG_testsum.mean(axis=0)[:210]\n",
    "IG2_testmean = IG_testsum.mean(axis=0)[210:420]\n",
    "IG3_testmean = IG_testsum.mean(axis=0)[420:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(join(main_path,'result/latent_210/main_result/AE', 'IG_attr_sub1_test.npy'), attr_sub1)\n",
    "np.save(join(main_path,'result/latent_210/main_result/AE', 'IG_attr_sub2_test.npy'), attr_sub2)\n",
    "np.save(join(main_path,'result/latent_210/main_result/AE', 'IG_attr_sub3_test.npy'), attr_sub3)\n",
    "np.save(join(main_path,'result/latent_210/main_result/AE', 'IG_testmean.npy'), IG_testmean)\n",
    "np.save(join(main_path,'result/latent_210/main_result/AE', 'IG_testsum.npy'), IG_testsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-norfolk",
   "metadata": {},
   "source": [
    "# IG visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_work = join(main_path,'result/latent_210')\n",
    "\n",
    "yeo7net_withSub_label = np.load(join(path_work, 'main_result', 'AE', 'BNA2yeo7.npy'), allow_pickle = True).item()['Label_name']\n",
    "BNA2yeo7_withSub_idx = np.load(join(path_work, 'main_result', 'AE', 'BNA2yeo7.npy'), allow_pickle = True).item()['BNA2yeo7_withSub_idx']\n",
    "\n",
    "yeo7net = yeo7net_withSub_label[:7]\n",
    "BNA2yeo7 = BNA2yeo7_withSub_idx[:210]\n",
    "\n",
    "data_list = [IG1_testmean, IG2_testmean, IG3_testmean]\n",
    "\n",
    "value_set = 'neg' # abs pos neg\n",
    "\n",
    "for i in data_list:\n",
    "\n",
    "#     data = np.load(join(path_work, 'main_result/AE', f'{i}')) # for MANOVA\n",
    "    data = i # for DC\n",
    "    \n",
    "    if value_set == 'abs':\n",
    "        data_yeo = np.array([abs(data)[BNA2yeo7==i].mean() for i in range(BNA2yeo7.max()+1)])\n",
    "        \n",
    "    if value_set == 'pos':\n",
    "        data_yeo = np.array([data[BNA2yeo7==i][data[BNA2yeo7==i]>0].mean() for i in range(BNA2yeo7.max()+1)])\n",
    "        \n",
    "    if value_set == 'neg':\n",
    "        data_yeo = np.array([data[BNA2yeo7==i][data[BNA2yeo7==i]<0].mean() for i in range(BNA2yeo7.max()+1)])\n",
    "        \n",
    "    print(data_yeo)\n",
    "    plot_spider(data_yeo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-sympathy",
   "metadata": {},
   "source": [
    "# Statistical analysis (MANOVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_otest_idx = np.load(join(main_path,'result/latent_210/main_result/AE/subj_idx.npy'), allow_pickle=True).item()['otest_idx']\n",
    "AE_cluster_label = np.load(join(main_path,'result/latent_210/main_result/AE/AE_cluster_label_idx.npy'))\n",
    "\n",
    "ME_otest_idx = np.load(join(main_path,'result/latent_210/main_result/ME/subj_idx.npy'), allow_pickle=True).item()['otest_idx']\n",
    "ME_cluster_label = np.load(join(main_path,'result/latent_210/main_result/ME/ME_cluster_label_idx.npy'))\n",
    "\n",
    "grad_list_znorm_stacked_regout = np.load(join(main_path,'result/latent_210/main_result/AE/grad_list_znorm_stacked_regout.npy'))\n",
    "\n",
    "IG_testsum = np.load(join('/store4/hschoi/1.Eat.DNN/result/latent_210/main_result/AE','IG', 'IG_testsum.npy'))\n",
    "# IG_testmean = np.load(join('/store4/hschoi/1.Eat.DNN/result/latent_210/main_result/AE', 'IG', 'IG_testmean.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IG \n",
    "\n",
    "import statsmodels\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "\n",
    "c_label = AE_cluster_label\n",
    "\n",
    "sub1 = np.where(c_label==0)[0]\n",
    "sub2 = np.where(c_label==1)[0]\n",
    "sub3 = np.where(c_label==2)[0]\n",
    "\n",
    "sub_label = np.zeros(len(c_label))\n",
    "sub_label[sub1] = 1\n",
    "sub_label[sub2] = 2\n",
    "sub_label[sub3] = 3\n",
    "\n",
    "df1 = pd.DataFrame(IG_testsum)\n",
    "df1 = df1.add_prefix('ROI_')\n",
    "df2 = pd.DataFrame(sub_label)\n",
    "df2.columns = ['Label']\n",
    "\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "# df[['ROI_0','ROI_210','ROI_420', 'Label']]\n",
    "\n",
    "label1 = df['Label'] == 1\n",
    "label2 = df['Label'] == 2\n",
    "label3 = df['Label'] == 3\n",
    "\n",
    "label_12 = df['Label'] !=3\n",
    "label_13 = df['Label'] !=2\n",
    "label_23 = df['Label'] !=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "from statsmodels.stats.multivariate import test_mvmean_2indep\n",
    "# Hotelling's t2 (Multivariate t-test)\n",
    "\n",
    "f_list = []\n",
    "p_list = []\n",
    "t2_list = []\n",
    "t_list = []\n",
    "Maha_dist = []\n",
    "\n",
    "l1_idx = label1 # label1 label1 label2\n",
    "l2_idx = label2 # label2 label3 label3\n",
    "\n",
    "formula_idx = [[f'ROI_{i}', f'ROI_{i+210}', f'ROI_{i+420}'] for i in range(210)]\n",
    "\n",
    "for i in range(len(formula_idx)):\n",
    "    x = df[formula_idx[i]][l1_idx]\n",
    "    y = df[formula_idx[i]][l2_idx]\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    \n",
    "    f_val = test_mvmean_2indep(x, y).statistic\n",
    "    p_val = test_mvmean_2indep(x, y).pvalue\n",
    "    t2_val = test_mvmean_2indep(x, y).t2\n",
    "    t_val = t2_val**(1/2)\n",
    "    Mahalanobis_dist = (nx+ny)/(nx*ny)*t2_val # Hotelling's t2 effect size\n",
    "    \n",
    "    f_list.append(f_val)\n",
    "    p_list.append(p_val)\n",
    "    t2_list.append(t2_val)\n",
    "    t_list.append(t_val)\n",
    "    Maha_dist.append(Mahalanobis_dist)\n",
    "    \n",
    "p_fdr = sm.stats.multitest.multipletests(p_list,alpha=0.01,method='fdr_bh')\n",
    "print('\\n')\n",
    "print(len(np.where(p_fdr[0]==True)[0]), '개')\n",
    "print('')\n",
    "print(np.where(p_fdr[0]==True)[0])\n",
    "print('')\n",
    "print(np.round(p_fdr[1][np.where(p_fdr[0]==True)],5))\n",
    "\n",
    "sig_idx = np.where(p_fdr[0]==True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-feeling",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statsmodels as sm\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "# Multivariate ANOVA\n",
    "\n",
    "f0_list = []\n",
    "eta_list = []\n",
    "p0_list = []\n",
    "\n",
    "formula_idx = [f'ROI_{i} + ROI_{i+210} + ROI_{i+420} ~ Label' for i in range(210)]\n",
    "\n",
    "for i in range(len(formula_idx)):\n",
    "    print(i, ' ', end = '', flush = True)\n",
    "    data = df # df \n",
    "    maov = MANOVA.from_formula(formula_idx[i], data = data) # df df[label_01] df[label_02] df[label_12]\n",
    "    # print(maov.mv_test())\n",
    "    f0 = maov.mv_test().results['Label']['stat']['F Value'].values[1] # Pillai's trace F value\n",
    "    v0 = maov.mv_test().results['Label']['stat']['Value'].values[1] # Pillai's trace V value\n",
    "    p0 = maov.mv_test().results['Label']['stat']['Pr > F'].values[1]\n",
    "    df1 = maov.mv_test().results['Label']['stat']['Num DF'].values[2]\n",
    "    df2 = maov.mv_test().results['Label']['stat']['Den DF'].values[2]\n",
    "    eta_squared = (df1*f0)/(df1*f0+df2) # MANOVA F effect size\n",
    "\n",
    "    f0_list.append(f0)\n",
    "    eta_list.append(eta_squared)\n",
    "    p0_list.append(p0)\n",
    "\n",
    "    \n",
    "p_0_fdr = sm.stats.multitest.multipletests(p0_list,alpha=0.05,method='fdr_bh')\n",
    "print('\\n')\n",
    "print(len(np.where(p_0_fdr[0]==True)[0]), '개')\n",
    "print('')\n",
    "print(np.where(p_0_fdr[0]==True)[0])\n",
    "print('')\n",
    "print(np.round(p_0_fdr[1][np.where(p_0_fdr[0]==True)],5))\n",
    "\n",
    "sig_idx = np.where(p_0_fdr[0]==True)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
