{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indian-meter",
   "metadata": {},
   "source": [
    "# Gradient to Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_name(0)) \n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "GPU_NUM = 1 # 원하는 GPU 번호 입력\n",
    "\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataloader\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "grad_list_znorm_stacked_regout = np.load(join(main_path,'result','latent_210', 'main_result/AE', 'grad_list_znorm_stacked_regout.npy'))\n",
    "\n",
    "\n",
    "x_data = torch.tensor(grad_list_znorm_stacked_regout) # FC_list_vec_threshed FC_list_vec_removed FC_list_vec FC_list_vec_removed_choice\n",
    "y_data = torch.tensor(y_scaled) # y_scaled y_choice_scaled\n",
    "    \n",
    "nsbj = len(x_data)\n",
    "\n",
    "# list_idx = np.random.permutation(np.arange(nsbj))\n",
    "itrain_idx = list_idx[:int(nsbj*0.6)]         # 100 subjects for training\n",
    "itest_idx = list_idx[int(nsbj*0.6):int(nsbj*0.8)]       # 10  subjects for validation\n",
    "otest_idx = list_idx[int(nsbj*0.8):]          # left subjects for test\n",
    "\n",
    "grad_list_znorm_stacked_regout_lemon = np.load(join(main_path,'result','latent_210', 'validation', 'grad_list_znorm_stacked_regout.npy'))\n",
    "lemon_data = torch.tensor(grad_list_znorm_stacked_regout_lemon)\n",
    "                                               \n",
    "x_train = x_data[itrain_idx]\n",
    "x_valid = x_data[itest_idx]\n",
    "x_test = x_data[otest_idx]\n",
    "x_lemon = lemon_data\n",
    "\n",
    "y_train = y_data[itrain_idx]\n",
    "y_valid = y_data[itest_idx]\n",
    "y_test = y_data[otest_idx]\n",
    "y_lemon = torch.tensor(np.zeros(len(lemon_data)))\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "lemon_ds = TensorDataset(x_lemon, y_lemon)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch_geometric.data import DataLoader\n",
    "\n",
    "bs_train = 10\n",
    "bs_valid = 10\n",
    "bs_test = 10\n",
    "bs_lemon = 10\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=bs_train, shuffle=False)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=bs_valid, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=bs_test, shuffle=False)\n",
    "lemon_loader = DataLoader(lemon_ds, batch_size=bs_lemon, shuffle=False)\n",
    "\n",
    "print(f'train : {len(itrain_idx)}, valid : {len(itest_idx)}, test : {len(otest_idx)}, lemon : {len(lemon_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-phone",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model construct\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "    \n",
    "    \n",
    "fea_orig = 630 \n",
    "fea1 = 420 \n",
    "fea2 = 210 \n",
    "fea3 = 120\n",
    "    \n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(fea_orig, fea1),\n",
    "            nn.Tanh())\n",
    "#             nn.Dropout(p = 0.3))\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(fea1, fea2),\n",
    "            nn.Tanh())\n",
    "#             nn.Dropout(p = 0.4))\n",
    "#         self.encoder3 = nn.Sequential(\n",
    "#             nn.Linear(fea2, fea3),\n",
    "#             nn.Tanh())\n",
    "\n",
    "#         self.decoder3 = nn.Sequential(\n",
    "#             nn.Linear(fea3, fea2),\n",
    "#             nn.Tanh())\n",
    "# # #             nn.Dropout(p = 0.2))\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Linear(fea2, fea1),\n",
    "            nn.Tanh())\n",
    "#             nn.Dropout(p = 0.3))\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Linear(fea1, fea_orig))\n",
    "#             nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        latent = self.encoder2(e1)\n",
    "#         latent = self.encoder3(e2)\n",
    "\n",
    "#         d3 = self.decoder3(latent)\n",
    "        d2 = self.decoder2(latent)\n",
    "        x = self.decoder1(d2)\n",
    "        \n",
    "        return x, latent\n",
    "\n",
    "class AutoEncoder_en(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder_en, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(fea_orig, fea1),\n",
    "            nn.Tanh())\n",
    "#             nn.Dropout(p = 0.3))\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(fea1, fea2),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        latent = self.encoder2(e1)\n",
    "        \n",
    "        return latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-necklace",
   "metadata": {},
   "source": [
    "# Train validate test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader, validloader, layer, loss_show = True):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_KLD_loss = 0.0\n",
    "    \n",
    "    y_list = []\n",
    "    y_hat_list = []\n",
    "    x_list = []\n",
    "    x_hat_list = []\n",
    "    recon_loss_list = []\n",
    "    reg_loss_list = []\n",
    "    vae_loss_list = []\n",
    "    \n",
    "    total_predict = []\n",
    "    total_y_pred = []\n",
    "    cla_loss_list = []\n",
    "    latent_train_list = []\n",
    "    \n",
    "    alpha = 1\n",
    "    for i, data in enumerate(dataloader):\n",
    "        x = data[0]\n",
    "        x = x.to(device)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "#         y = data[1]\n",
    "#         y = y.to(device)\n",
    "#         y = y.view(y.size(0), -1)\n",
    "#             print('y',y)\n",
    "        if layer == 'AE':\n",
    "            x_hat, latent_train = model(x.float())\n",
    "            recon_loss = criterion(x.float(), x_hat.float())\n",
    "#             sparsity = sparse_loss(RHO, x.float())\n",
    "            loss = recon_loss #+ sparsity * 0.1  # + criterion(output_e1.float(), output_d1.float()) + criterion(output_e2.float(), output_d2.float()) + criterion_reg(y.float(), y_hat.float())\n",
    "    #             print(f'loss : {loss}')\n",
    "#             print(f'recon_loss : {recon_loss}, sparsity : {sparsity}')\n",
    "\n",
    "            x_list.append(x)\n",
    "            x_hat_list.append(x_hat)\n",
    "            recon_loss_list.append(recon_loss.item())\n",
    "            latent_train_list.append(latent_train)\n",
    "    #         del y\n",
    "\n",
    "             \n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        loss.backward(retain_graph = True)\n",
    "        optimizer.step()\n",
    "#         lr_decay.step()\n",
    "\n",
    "#         print('output',output[:,:3])\n",
    "#         print('data',data[:,:3])\n",
    "\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    if loss_show:\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")        \n",
    "    \n",
    "    val_loss, x_list_valid, x_hat_list_valid, recon_loss_list_valid, latent_valid_list = validate(model, validloader, layer)\n",
    "    if loss_show:\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return train_loss, val_loss, x_list, x_hat_list, x_list_valid, x_hat_list_valid, recon_loss_list, recon_loss_list_valid, latent_train_list, latent_valid_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, layer):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    y_list = []\n",
    "    y_hat_list = []\n",
    "    x_hat_list = []\n",
    "    x_list = []\n",
    "    x_hat_list = []\n",
    "    recon_loss_list = []\n",
    "    reg_loss_list = []\n",
    "    cla_loss_list = []\n",
    "    vae_loss_list = []\n",
    "    \n",
    "    total_predict = []\n",
    "    total_y_pred = []\n",
    "    latent_valid_list = []\n",
    "    \n",
    "    alpha = 1\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            x = data[0]\n",
    "            x = x.to(device)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            \n",
    "            \n",
    "#             y = data[1]\n",
    "#             y = y.to(device)\n",
    "#             y = y.view(y.size(0), -1)\n",
    "            if layer == 'AE':\n",
    "                x_hat, latent_valid = model(x.float())\n",
    "                recon_loss = criterion(x.float(), x_hat.float())\n",
    "#                 sparsity = sparse_loss(RHO, x.float())\n",
    "    #                 reg_loss = criterion_reg(y.float(), y_hat.float())\n",
    "                loss = recon_loss #+ sparsity*0.1 # + criterion(output_e1.float(), output_d1.float()) + criterion(output_e2.float(), output_d2.float()) + criterion_reg(y.float(), y_hat.float())\n",
    "    #                 print('recon_loss valid ', recon_loss)\n",
    "\n",
    "                x_list.append(x)\n",
    "                x_hat_list.append(x_hat)\n",
    "                recon_loss_list.append(recon_loss.item())\n",
    "                latent_valid_list.append(latent_valid)\n",
    "    #                 reg_loss_list.append(reg_loss.item())\n",
    "    #             del y\n",
    "            \n",
    "            del x\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    \n",
    "    \n",
    "    return val_loss, x_list, x_hat_list, recon_loss_list, latent_valid_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-amber",
   "metadata": {},
   "source": [
    "# Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-denial",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "layer = 'AE'\n",
    "\n",
    "if layer == 'AE':\n",
    "    model = AutoEncoder().to(device) # AutoEncoder \n",
    "\n",
    "model_children = list(model.children())\n",
    "\n",
    "epochs = 500\n",
    "lr = 0.0001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "lr_decay = lr_scheduler.StepLR(optimizer, step_size = 10 ,gamma=0.95)\n",
    "criterion = nn.MSELoss(reduction = 'sum')\n",
    "criterion_reg = nn.MSELoss(reduction = 'sum') # nn.CrossEntropyLoss() nn.MSELoss()\n",
    "\n",
    "save_path = join(main_path,'log')\n",
    "\n",
    "best_model = deepcopy(model)\n",
    "best_val_loss = np.inf\n",
    "\n",
    "\n",
    "\n",
    "train_loss_total = []\n",
    "val_loss_total = []\n",
    "recon_loss_total_train = []\n",
    "reg_loss_total_train = []\n",
    "recon_loss_total_valid = []\n",
    "reg_loss_total_valid = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, val_epoch_loss, x_train, x_hat_train, x_valid, x_hat_valid, recon_loss_list_train, recon_loss_list_valid, latent_train, latent_valid= fit(model, train_loader, valid_loader, layer, loss_show=True)\n",
    "        \n",
    "\n",
    "    train_loss_total.append(train_epoch_loss)\n",
    "    val_loss_total.append(val_epoch_loss)\n",
    "    recon_loss_total_train.append(recon_loss_list_train)\n",
    "#     reg_loss_total_train.append(reg_loss_list_train)\n",
    "    recon_loss_total_valid.append(recon_loss_list_valid)\n",
    "#     reg_loss_total_valid.append(reg_loss_list_valid)\n",
    "        \n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        print('[Updated New Record!]')\n",
    "        best_val_loss = val_epoch_loss\n",
    "        best_model = deepcopy(model)\n",
    "        best_epoch = epoch\n",
    "#         best_y_train = y_train\n",
    "#         best_y_hat_train = y_hat_train\n",
    "#         best_y_valid = y_valid\n",
    "#         best_y_hat_valid = y_hat_valid\n",
    "        best_x_train = x_train\n",
    "        best_x_hat_train = x_hat_train\n",
    "        best_x_valid = x_valid\n",
    "        best_x_hat_valid = x_hat_valid\n",
    "    print('')\n",
    "    \n",
    "    if epoch > best_epoch +50:\n",
    "        print('Stop!')\n",
    "        break\n",
    "\n",
    "    lr_decay.step()\n",
    "    \n",
    "# torch.save(best_model.state_dict(), save_path + '/' + trial + '_best_AE_epoch%d.pkl' % best_epoch)\n",
    "print(\"best AE epoch to %d\" % (best_epoch + 1))\n",
    "print(\"best AE val_loss to %.4f\" % best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = join(main_path,'log/grad.compare')\n",
    "trial = '01'\n",
    "\n",
    "torch.save(best_model.state_dict(), save_path + '/' + trial + '_best_AE_epoch%d.pkl' % best_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-georgia",
   "metadata": {},
   "source": [
    "# Train set latent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-supervision",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.matshow(torch.cat(latent_train).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-professor",
   "metadata": {},
   "source": [
    "# Reconstruction evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-elevation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.cat(x_train).detach().cpu().numpy()\n",
    "y = torch.cat(best_x_hat_train).detach().cpu().numpy()\n",
    "\n",
    "corr_train = np.array([sc.stats.pearsonr(x[i],y[i])[0] for i in range(len(x))])\n",
    "p_train = np.array([sc.stats.pearsonr(x[i],y[i])[1] for i in range(len(x))])\n",
    "\n",
    "corr_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat(x_valid).detach().cpu().numpy()\n",
    "y = torch.cat(best_x_hat_valid).detach().cpu().numpy()\n",
    "\n",
    "corr_valid = np.array([sc.stats.pearsonr(x[i],y[i])[0] for i in range(len(x))])\n",
    "p_valid = np.array([sc.stats.pearsonr(x[i],y[i])[1] for i in range(len(x))])\n",
    "\n",
    "corr_valid.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-colonial",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.matshow(torch.cat(x_train).detach().cpu().numpy()[:80])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.clim(-2.5,2.5)\n",
    "plt.colorbar(shrink=0.75)\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.matshow(torch.cat(best_x_hat_train).detach().cpu().numpy()[:80])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.clim(-2.5,2.5)\n",
    "plt.colorbar(shrink=0.75)\n",
    "\n",
    "plt.figure(3)\n",
    "plt.matshow(torch.cat(x_valid).detach().cpu().numpy())\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.clim(-2.5,2.5)\n",
    "plt.colorbar(shrink=0.75)\n",
    "\n",
    "plt.figure(4)\n",
    "plt.matshow(torch.cat(best_x_hat_valid).detach().cpu().numpy())\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.clim(-2.5,2.5)\n",
    "plt.colorbar(shrink=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-chick",
   "metadata": {},
   "source": [
    "# Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(x, y, label_x, label_y):\n",
    "    plt.figure(1, (7,5))\n",
    "    plt.plot(x, label = label_x, color= 'k', linewidth=5)\n",
    "    plt.plot(y, label = label_y, color = 'gray', linewidth=5)\n",
    "#     plt.xlabel('Epochs')\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.legend(fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-owner",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = train_loss_total\n",
    "y = val_loss_total\n",
    "\n",
    "label_x = 'Train loss'\n",
    "label_y = 'Valid loss'\n",
    "\n",
    "loss_plot(x, y, label_x, label_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
